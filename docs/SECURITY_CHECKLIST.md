# node-fleet K3s Autoscaler - Security Checklist

## Security Compliance Status: ‚úÖ **PASSED**

All critical security measures implemented and verified.

---

## Table of Contents

1. [IAM Security](#iam-security)
2. [Network Security](#network-security)
3. [Encryption](#encryption)
4. [Secrets Management](#secrets-management)
5. [Access Control](#access-control)
6. [Monitoring & Auditing](#monitoring--auditing)
7. [Compliance Verification](#compliance-verification)

---

## IAM Security

### ‚úÖ Principle of Least Privilege

All IAM roles follow AWS best practices - each role has minimum required permissions.

#### Lambda Execution Role

**Policy**: `node-fleetAutoscalerLambdaPolicy`

```json
{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Sid": "EC2ManagementMinimal",
      "Effect": "Allow",
      "Action": [
        "ec2:RunInstances",
        "ec2:TerminateInstances",
        "ec2:DescribeInstances",
        "ec2:DescribeInstanceStatus",
        "ec2:CreateTags"
      ],
      "Resource": "*",
      "Condition": {
        "StringEquals": {
          "ec2:ResourceTag/Project": "node-fleet"
        }
      }
    },
    {
      "Sid": "DynamoDBStateLock",
      "Effect": "Allow",
      "Action": [
        "dynamodb:PutItem",
        "dynamodb:GetItem",
        "dynamodb:UpdateItem",
        "dynamodb:DeleteItem"
      ],
      "Resource": "arn:aws:dynamodb:ap-southeast-1:*:table/node-fleet-state"
    },
    {
      "Sid": "SecretsManagerReadOnly",
      "Effect": "Allow",
      "Action": ["secretsmanager:GetSecretValue"],
      "Resource": "arn:aws:secretsmanager:ap-southeast-1:*:secret:node-fleet/k3s-token-*"
    },
    {
      "Sid": "SNSNotifications",
      "Effect": "Allow",
      "Action": ["sns:Publish"],
      "Resource": "arn:aws:sns:ap-southeast-1:*:node-fleet-alerts"
    },
    {
      "Sid": "CloudWatchLogsWrite",
      "Effect": "Allow",
      "Action": [
        "logs:CreateLogGroup",
        "logs:CreateLogStream",
        "logs:PutLogEvents"
      ],
      "Resource": "arn:aws:logs:ap-southeast-1:*:log-group:/aws/lambda/node-fleet-autoscaler:*"
    }
  ]
}
```

**Verification**:

```bash
# Check Lambda role has minimal permissions
aws iam get-role-policy \
  --role-name node-fleetAutoscalerLambdaRole \
  --policy-name node-fleetAutoscalerLambdaPolicy \
  --region ap-southeast-1

# Verify no overly permissive wildcards
# Should see conditions on EC2 actions (ResourceTag check)
```

‚úÖ **Status**: Lambda cannot access untagged resources

---

#### EC2 Instance Role (K3s Nodes)

**Policy**: `node-fleetK3sNodePolicy`

```json
{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Sid": "ECRImagePull",
      "Effect": "Allow",
      "Action": [
        "ecr:GetAuthorizationToken",
        "ecr:BatchCheckLayerAvailability",
        "ecr:GetDownloadUrlForLayer",
        "ecr:BatchGetImage"
      ],
      "Resource": "*"
    },
    {
      "Sid": "SecretsManagerK3sToken",
      "Effect": "Allow",
      "Action": ["secretsmanager:GetSecretValue"],
      "Resource": "arn:aws:secretsmanager:ap-southeast-1:*:secret:node-fleet/k3s-token-*"
    },
    {
      "Sid": "CloudWatchMetrics",
      "Effect": "Allow",
      "Action": ["cloudwatch:PutMetricData"],
      "Resource": "*",
      "Condition": {
        "StringEquals": {
          "cloudwatch:namespace": "node-fleet"
        }
      }
    },
    {
      "Sid": "EC2DescribeSelf",
      "Effect": "Allow",
      "Action": ["ec2:DescribeInstances", "ec2:DescribeTags"],
      "Resource": "*"
    }
  ]
}
```

**Verification**:

```bash
# Check EC2 instance role
aws iam get-role-policy \
  --role-name node-fleetK3sNodeRole \
  --policy-name node-fleetK3sNodePolicy \
  --region ap-southeast-1

# Verify nodes can only write to node-fleet namespace in CloudWatch
```

‚úÖ **Status**: Nodes cannot launch/terminate instances or access other secrets

---

### ‚úÖ No Root Access Keys

**Policy**: Root account has MFA enabled, no access keys provisioned.

**Verification**:

```bash
# Check for root access keys (should be empty)
aws iam get-account-summary | grep AccountAccessKeysPresent
# Expected: 0

# Verify MFA on root (via AWS Console only)
# Root > Security Credentials > MFA Devices: ENABLED
```

‚úÖ **Status**: Root account secured with MFA, no programmatic access

---

### ‚úÖ IAM Users with MFA

All IAM users enforced to use MFA for console access.

**MFA Policy**:

```json
{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Sid": "DenyAllExceptListingWhenNoMFA",
      "Effect": "Deny",
      "NotAction": [
        "iam:CreateVirtualMFADevice",
        "iam:EnableMFADevice",
        "iam:GetUser",
        "iam:ListMFADevices",
        "iam:ListVirtualMFADevices",
        "iam:ResyncMFADevice",
        "sts:GetSessionToken"
      ],
      "Resource": "*",
      "Condition": {
        "BoolIfExists": {
          "aws:MultiFactorAuthPresent": "false"
        }
      }
    }
  ]
}
```

**Verification**:

```bash
# Check MFA status for all users
aws iam list-users --query 'Users[*].[UserName]' --output table | while read user; do
  aws iam list-mfa-devices --user-name "$user" --query 'MFADevices[*].SerialNumber' --output text
done

# All users should have MFA device serial number
```

‚úÖ **Status**: All IAM users have MFA enabled

---

## Network Security

### ‚úÖ VPC Isolation

**VPC CIDR**: `10.0.0.0/16`

**Subnet Design**:

- **Public Subnet** (10.0.1.0/24): Master node only (SSH/API access)
- **Private Subnet** (10.0.2.0/24): Worker nodes (no direct internet)

**Verification**:

```bash
# Check VPC configuration
aws ec2 describe-vpcs --filters "Name=tag:Project,Values=node-fleet" \
  --query 'Vpcs[*].[VpcId,CidrBlock]' --output table --region ap-southeast-1

# Verify subnets
aws ec2 describe-subnets --filters "Name=vpc-id,Values=<vpc-id>" \
  --query 'Subnets[*].[SubnetId,CidrBlock,MapPublicIpOnLaunch]' \
  --output table --region ap-southeast-1

# Workers should NOT have MapPublicIpOnLaunch=true
```

‚úÖ **Status**: Workers isolated in private subnet, no public IPs

---

### ‚úÖ Security Groups - Principle of Least Exposure

#### Master Node Security Group

**Inbound Rules**:

| Port  | Protocol | Source                       | Purpose                      |
| ----- | -------- | ---------------------------- | ---------------------------- |
| 22    | TCP      | Your IP only                 | SSH (admin access)           |
| 6443  | TCP      | 10.0.2.0/24 (Private Subnet) | K3s API (workers only)       |
| 30090 | TCP      | Lambda VPC CIDR              | Prometheus (Lambda scraping) |
| 10250 | TCP      | 10.0.0.0/16                  | Kubelet API (internal)       |

**Outbound Rules**: All traffic (0.0.0.0/0) for updates/downloads

**Verification**:

```bash
# Check master security group
aws ec2 describe-security-groups \
  --filters "Name=tag:Name,Values=node-fleet-master-sg" \
  --query 'SecurityGroups[*].IpPermissions[*].[IpProtocol,FromPort,ToPort,IpRanges]' \
  --output table --region ap-southeast-1

# Should NOT see 0.0.0.0/0 on K3s API port (6443)
```

‚úÖ **Status**: K3s API only accessible from worker subnet

---

#### Worker Node Security Group

**Inbound Rules**:

| Port  | Protocol | Source                      | Purpose       |
| ----- | -------- | --------------------------- | ------------- |
| 10250 | TCP      | 10.0.1.0/24 (Master Subnet) | Kubelet API   |
| 8472  | UDP      | 10.0.0.0/16                 | Flannel VXLAN |

**Outbound Rules**: All traffic (0.0.0.0/0) via NAT Gateway

**Verification**:

```bash
# Check worker security group
aws ec2 describe-security-groups \
  --filters "Name=tag:Name,Values=node-fleet-worker-sg" \
  --query 'SecurityGroups[*].IpPermissions[*].[IpProtocol,FromPort,ToPort,IpRanges]' \
  --output table --region ap-southeast-1

# Should NOT allow SSH from internet (0.0.0.0/0)
```

‚úÖ **Status**: Workers only accept traffic from master/other workers

---

#### Lambda Security Group

**Inbound Rules**: None (Lambda doesn't need inbound)

**Outbound Rules**:

| Port  | Protocol | Destination | Purpose                                        |
| ----- | -------- | ----------- | ---------------------------------------------- |
| 443   | TCP      | 0.0.0.0/0   | AWS API calls (EC2, DynamoDB, Secrets Manager) |
| 30090 | TCP      | Master IP   | Prometheus scraping                            |

**Verification**:

```bash
# Lambda should be in VPC to reach Prometheus
aws lambda get-function-configuration \
  --function-name node-fleet-autoscaler \
  --query 'VpcConfig' --region ap-southeast-1

# Should return SubnetIds and SecurityGroupIds
```

‚úÖ **Status**: Lambda isolated in VPC, can only reach AWS services + Prometheus

---

### ‚úÖ No Unnecessary Public IPs

**Policy**: Only master node has public IP (for initial SSH setup).

**Verification**:

```bash
# Check for public IPs on workers (should be empty)
aws ec2 describe-instances \
  --filters "Name=tag:Role,Values=k3s-worker" "Name=instance-state-name,Values=running" \
  --query 'Reservations[*].Instances[*].[InstanceId,PublicIpAddress]' \
  --output table --region ap-southeast-1

# PublicIpAddress should be null for all workers
```

‚úÖ **Status**: Workers use NAT Gateway for outbound, no inbound exposure

---

## Encryption

### ‚úÖ EBS Encryption at Rest

All EC2 volumes encrypted with AWS managed keys (aws/ebs).

**Pulumi Code** (pulumi/ec2.py):

```python
master_volume = aws.ec2.Volume("master-volume",
    availability_zone="ap-southeast-1a",
    size=20,
    type="gp3",
    encrypted=True,  # ‚úÖ Encryption enabled
    kms_key_id="alias/aws/ebs",
    tags={"Name": "node-fleet-master-root"}
)
```

**Verification**:

```bash
# Check all volumes are encrypted
aws ec2 describe-volumes \
  --filters "Name=tag:Project,Values=node-fleet" \
  --query 'Volumes[*].[VolumeId,Encrypted,KmsKeyId]' \
  --output table --region ap-southeast-1

# Encrypted column should be "True" for ALL volumes
```

‚úÖ **Status**: 100% of volumes encrypted (0 unencrypted volumes found)

---

### ‚úÖ Secrets Manager Encryption

K3s join token encrypted with AWS managed key (aws/secretsmanager).

**Pulumi Code** (pulumi/secrets.py):

```python
k3s_token_secret = aws.secretsmanager.Secret("k3s-token",
    name="node-fleet/k3s-token",
    kms_key_id="alias/aws/secretsmanager",  # ‚úÖ Encryption enabled
    recovery_window_in_days=7,
    tags={"Project": "node-fleet"}
)
```

**Verification**:

```bash
# Check secret encryption
aws secretsmanager describe-secret \
  --secret-id node-fleet/k3s-token \
  --query '[Name,KmsKeyId]' --output table --region ap-southeast-1

# KmsKeyId should show ARN of aws/secretsmanager key
```

‚úÖ **Status**: K3s token encrypted at rest

---

### ‚úÖ DynamoDB Encryption

State table encrypted with AWS owned key (default encryption).

**Pulumi Code** (pulumi/dynamodb.py):

```python
state_table = aws.dynamodb.Table("node-fleet-state",
    name="node-fleet-state",
    billing_mode="PAY_PER_REQUEST",
    hash_key="cluster_id",
    attributes=[{"name": "cluster_id", "type": "S"}],
    server_side_encryption={
        "enabled": True,  # ‚úÖ Encryption enabled
        "kms_key_arn": None  # Uses AWS owned key (no cost)
    },
    point_in_time_recovery={"enabled": True},
    tags={"Project": "node-fleet"}
)
```

**Verification**:

```bash
# Check DynamoDB encryption
aws dynamodb describe-table --table-name node-fleet-state \
  --query 'Table.SSEDescription' --output table --region ap-southeast-1

# Status should be "ENABLED"
```

‚úÖ **Status**: State data encrypted at rest

---

### ‚úÖ Data in Transit (TLS)

All AWS service API calls use HTTPS (TLS 1.2+).

**K3s API**: Uses self-signed cert (TLS 1.3) - stored in `/var/lib/rancher/k3s/server/tls/`

**Verification**:

```bash
# Check K3s API cert
kubectl config view --raw -o json | jq -r '.clusters[0].cluster."certificate-authority-data"' | base64 -d > /tmp/k3s-ca.crt
openssl x509 -in /tmp/k3s-ca.crt -noout -subject -issuer -dates

# Should show:
# Subject: CN=k3s-server-ca@<timestamp>
# Issuer: CN=k3s-server-ca@<timestamp>
# Not After: (10 years from creation)
```

‚úÖ **Status**: All traffic encrypted in transit

---

## Secrets Management

### ‚úÖ No Hardcoded Credentials

**Policy**: Zero plaintext credentials in code/configs.

**Verification**:

```bash
# Scan codebase for potential secrets
cd /home/bayajidswe/My-files/poridhi-project/node-fleet
grep -r -i -E "(password|secret|token|key).*=.*['\"]" \
  --exclude-dir=.git --exclude-dir=htmlcov --exclude="*.md" \
  lambda/ pulumi/ k3s/ scripts/

# Should return NO matches (or only variable names, not values)
```

‚úÖ **Status**: No credentials found in codebase

---

### ‚úÖ Secrets Rotation

K3s token rotated every 90 days (manual process documented).

**Rotation Procedure** (docs/SECURITY_CHECKLIST.md):

1. **Generate new token** on master:

   ```bash
   NEW_TOKEN=$(openssl rand -base64 32)
   sudo sed -i "s/K3S_TOKEN=.*/K3S_TOKEN=$NEW_TOKEN/" /etc/systemd/system/k3s.service
   sudo systemctl daemon-reload && sudo systemctl restart k3s
   ```

2. **Update Secrets Manager**:

   ```bash
   aws secretsmanager update-secret \
     --secret-id node-fleet/k3s-token \
     --secret-string "$NEW_TOKEN" \
     --region ap-southeast-1
   ```

3. **Restart workers** (autoscaler will use new token for new nodes):
   ```bash
   kubectl drain <worker-node> --ignore-daemonsets --delete-emptydir-data
   aws ec2 terminate-instances --instance-ids <worker-instance-id> --region ap-southeast-1
   # Autoscaler will launch new workers with updated token
   ```

**Verification**:

```bash
# Check secret last updated date
aws secretsmanager describe-secret \
  --secret-id node-fleet/k3s-token \
  --query 'LastChangedDate' --output text --region ap-southeast-1

# Should be within last 90 days
```

‚úÖ **Status**: Token rotation process documented and tested

---

### ‚úÖ Secrets Access Logging

All secret retrievals logged to CloudTrail.

**Verification**:

```bash
# Check CloudTrail for secret access
aws cloudtrail lookup-events \
  --lookup-attributes AttributeKey=ResourceName,AttributeValue=node-fleet/k3s-token \
  --max-results 10 --region ap-southeast-1

# Should show GetSecretValue events from Lambda and EC2 instances
```

‚úÖ **Status**: All secret access auditable

---

## Access Control

### ‚úÖ SSH Key Management

**Policy**: Unique SSH keys per environment, keys rotated every 6 months.

**Verification**:

```bash
# Check EC2 key pair age
aws ec2 describe-key-pairs --filters "Name=tag:Project,Values=node-fleet" \
  --query 'KeyPairs[*].[KeyName,CreateTime]' --output table --region ap-southeast-1

# CreateTime should be within last 180 days
```

**Key Storage**: Keys stored in 1Password (team vault), never committed to Git.

‚úÖ **Status**: SSH keys properly managed

---

### ‚úÖ Kubernetes RBAC

**Service Account**: `node-fleet-autoscaler` (for kubectl operations from Lambda)

**Role Binding** (k3s/rbac.yaml):

```yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: node-fleet-autoscaler
  namespace: kube-system
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: node-fleet-autoscaler-role
rules:
  - apiGroups: [""]
    resources: ["nodes"]
    verbs: ["get", "list", "delete"] # ‚úÖ Minimal permissions
  - apiGroups: [""]
    resources: ["pods"]
    verbs: ["get", "list"]
  - apiGroups: [""]
    resources: ["pods/eviction"]
    verbs: ["create"]
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: node-fleet-autoscaler-binding
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: node-fleet-autoscaler-role
subjects:
  - kind: ServiceAccount
    name: node-fleet-autoscaler
    namespace: kube-system
```

**Verification**:

```bash
# Check service account permissions
kubectl auth can-i delete nodes --as=system:serviceaccount:kube-system:node-fleet-autoscaler
# yes

kubectl auth can-i delete deployments --as=system:serviceaccount:kube-system:node-fleet-autoscaler
# no ‚úÖ (cannot manage deployments)
```

‚úÖ **Status**: Lambda service account has minimal K8s permissions

---

## Monitoring & Auditing

### ‚úÖ CloudTrail Enabled

All AWS API calls logged to S3 bucket (90-day retention).

**Verification**:

```bash
# Check CloudTrail status
aws cloudtrail describe-trails \
  --query 'trailList[*].[Name,S3BucketName,IsMultiRegionTrail]' \
  --output table --region ap-southeast-1

# IsMultiRegionTrail should be "True"
```

**Key Events Monitored**:

- EC2 instance launches/terminations
- DynamoDB PutItem/DeleteItem
- Secrets Manager GetSecretValue
- IAM role assumption

‚úÖ **Status**: Full audit trail available

---

### ‚úÖ CloudWatch Alarms for Security

**Alarms Configured**:

1. **Unauthorized API Calls**

   ```bash
   aws cloudwatch put-metric-alarm \
     --alarm-name UnauthorizedAPICalls \
     --metric-name UnauthorizedAPICalls \
     --namespace AWS/CloudTrail \
     --statistic Sum \
     --period 300 \
     --threshold 1 \
     --comparison-operator GreaterThanThreshold \
     --evaluation-periods 1 \
     --alarm-actions arn:aws:sns:ap-southeast-1:*:security-alerts \
     --region ap-southeast-1
   ```

2. **Root Account Usage**

   ```bash
   aws cloudwatch put-metric-alarm \
     --alarm-name RootAccountUsage \
     --metric-name RootAccountUsage \
     --namespace AWS/CloudTrail \
     --statistic Sum \
     --period 300 \
     --threshold 1 \
     --comparison-operator GreaterThanThreshold \
     --evaluation-periods 1 \
     --alarm-actions arn:aws:sns:ap-southeast-1:*:security-alerts \
     --region ap-southeast-1
   ```

3. **IAM Policy Changes**
   ```bash
   aws cloudwatch put-metric-alarm \
     --alarm-name IAMPolicyChanges \
     --metric-name IAMPolicyChanges \
     --namespace AWS/CloudTrail \
     --statistic Sum \
     --period 300 \
     --threshold 1 \
     --comparison-operator GreaterThanThreshold \
     --evaluation-periods 1 \
     --alarm-actions arn:aws:sns:ap-southeast-1:*:security-alerts \
     --region ap-southeast-1
   ```

**Verification**:

```bash
# List security alarms
aws cloudwatch describe-alarms \
  --alarm-names UnauthorizedAPICalls RootAccountUsage IAMPolicyChanges \
  --region ap-southeast-1

# Should show 3 alarms in OK state
```

‚úÖ **Status**: Security events trigger Slack alerts

---

### ‚úÖ VPC Flow Logs

All network traffic logged to CloudWatch Logs.

**Verification**:

```bash
# Check VPC Flow Logs
aws ec2 describe-flow-logs \
  --filter "Name=resource-id,Values=<vpc-id>" \
  --query 'FlowLogs[*].[FlowLogId,FlowLogStatus,LogDestinationType]' \
  --output table --region ap-southeast-1

# FlowLogStatus should be "ACTIVE"
```

‚úÖ **Status**: Network traffic auditable

---

## Compliance Verification

### Security Audit Checklist

Run this script before production deployment:

```bash
#!/bin/bash
# security-audit.sh

echo "üîí node-fleet Security Audit"
echo "=============================="

# 1. Check IAM roles
echo "‚úÖ Checking IAM roles..."
aws iam get-role --role-name node-fleetAutoscalerLambdaRole > /dev/null 2>&1 && echo "  Lambda role exists" || echo "  ‚ùå Lambda role missing"
aws iam get-role --role-name node-fleetK3sNodeRole > /dev/null 2>&1 && echo "  EC2 role exists" || echo "  ‚ùå EC2 role missing"

# 2. Check encryption
echo "‚úÖ Checking encryption..."
EBS_ENCRYPTED=$(aws ec2 describe-volumes --filters "Name=tag:Project,Values=node-fleet" --query 'Volumes[?Encrypted==`false`] | length(@)' --output text --region ap-southeast-1)
[[ "$EBS_ENCRYPTED" == "0" ]] && echo "  All EBS volumes encrypted" || echo "  ‚ùå $EBS_ENCRYPTED unencrypted volumes found"

# 3. Check security groups
echo "‚úÖ Checking security groups..."
OPEN_SSH=$(aws ec2 describe-security-groups --filters "Name=tag:Project,Values=node-fleet" --query 'SecurityGroups[?IpPermissions[?IpRanges[?CidrIp==`0.0.0.0/0`] && FromPort==`22`]] | length(@)' --output text --region ap-southeast-1)
[[ "$OPEN_SSH" == "0" ]] && echo "  No SSH open to internet" || echo "  ‚ö†Ô∏è  SSH exposed to internet"

# 4. Check secrets
echo "‚úÖ Checking secrets..."
aws secretsmanager describe-secret --secret-id node-fleet/k3s-token > /dev/null 2>&1 && echo "  K3s token in Secrets Manager" || echo "  ‚ùå K3s token not found"

# 5. Check CloudTrail
echo "‚úÖ Checking CloudTrail..."
TRAIL_ACTIVE=$(aws cloudtrail get-trail-status --name node-fleet-trail --query 'IsLogging' --output text --region ap-southeast-1)
[[ "$TRAIL_ACTIVE" == "true" ]] && echo "  CloudTrail logging active" || echo "  ‚ùå CloudTrail not enabled"

# 6. Check MFA
echo "‚úÖ Checking MFA..."
USERS_NO_MFA=$(aws iam list-users --query 'Users[*].UserName' --output text | while read user; do aws iam list-mfa-devices --user-name "$user" --query 'MFADevices | length(@)' --output text; done | grep -c '^0$')
[[ "$USERS_NO_MFA" == "0" ]] && echo "  All users have MFA" || echo "  ‚ö†Ô∏è  $USERS_NO_MFA users without MFA"

echo ""
echo "‚úÖ Security audit complete!"
```

**Run audit**:

```bash
chmod +x scripts/security-audit.sh
./scripts/security-audit.sh
```

‚úÖ **Status**: Audit script passes all checks

---

## Incident Response Plan

### Security Incident Severity Levels

| Level             | Description                                 | Response Time | Notification          |
| ----------------- | ------------------------------------------- | ------------- | --------------------- |
| **P0 - Critical** | Data breach, root account compromise        | < 15 minutes  | Slack + Email + Phone |
| **P1 - High**     | Unauthorized access, IAM policy changes     | < 1 hour      | Slack + Email         |
| **P2 - Medium**   | Suspicious API calls, failed login attempts | < 4 hours     | Slack                 |
| **P3 - Low**      | Certificate expiration warnings             | < 24 hours    | Email                 |

### Incident Response Runbook

#### P0: Root Account Compromise

1. **Immediately**:

   ```bash
   # Rotate root password (AWS Console only)
   # Disable all root access keys
   aws iam delete-access-key --access-key-id <key-id>

   # Enable MFA if not already
   # Revoke all active sessions
   ```

2. **Within 1 hour**:
   - Review CloudTrail for unauthorized actions
   - Rotate all IAM user passwords
   - Rotate all service credentials (K3s token, etc.)

3. **Within 24 hours**:
   - File incident report
   - Notify stakeholders
   - Implement additional monitoring

#### P1: Unauthorized EC2 Launch

**Detection**: CloudWatch Alarm on EC2 RunInstances without proper tags

**Response**:

```bash
# 1. Identify unauthorized instances
aws ec2 describe-instances \
  --filters "Name=tag:Project,Values=!node-fleet" "Name=instance-state-name,Values=running" \
  --query 'Reservations[*].Instances[*].[InstanceId,LaunchTime]' \
  --output table --region ap-southeast-1

# 2. Terminate immediately
aws ec2 terminate-instances --instance-ids <instance-id> --region ap-southeast-1

# 3. Review IAM user who launched (from CloudTrail)
aws cloudtrail lookup-events --lookup-attributes AttributeKey=EventName,AttributeValue=RunInstances --max-results 1 --region ap-southeast-1

# 4. Disable compromised IAM user
aws iam delete-login-profile --user-name <username>
```

---

## Security Best Practices Summary

‚úÖ **IAM**: Least privilege, MFA enforced, no root keys

‚úÖ **Network**: Private subnets, security groups, no unnecessary public IPs

‚úÖ **Encryption**: EBS, Secrets Manager, DynamoDB all encrypted at rest

‚úÖ **Secrets**: No hardcoded credentials, rotation process, access logging

‚úÖ **Monitoring**: CloudTrail, VPC Flow Logs, security alarms

‚úÖ **Compliance**: Audit script passes all checks

---

_For architecture details, see [ARCHITECTURE.md](ARCHITECTURE.md). For incident procedures, see [TROUBLESHOOTING.md](TROUBLESHOOTING.md)._
