# ğŸ§© SmartScale K3s Autoscaler - Final Project

> **TechFlow Solutions E-Commerce Platform Autoscaling System**  
> Intelligent K3s autoscaler that reduces infrastructure costs by 40-50% while improving reliability

[![Project Status](https://img.shields.io/badge/Status-In%20Development-yellow)]()
[![AWS](https://img.shields.io/badge/AWS-Cloud-orange)]()
[![K3s](https://img.shields.io/badge/K3s-Kubernetes-blue)]()
[![Python](https://img.shields.io/badge/Python-3.11-blue)]()

---

## ğŸ“‹ Project Overview

### Business Context
TechFlow Solutions is a Dhaka-based e-commerce startup serving **15,000+ daily users** and processing **80 lakh BDT in monthly transactions**. The platform currently runs on a fixed 5-node K3s cluster costing **1.2 lakh BDT/month**, with:

- **60,000 BDT/month wasted** on unused capacity during off-peak hours
- **8 lakh BDT lost** in a recent flash sale crash due to insufficient capacity
- **15-20 minute manual scaling** process causing service degradation

### Solution
An intelligent, automated autoscaling system that:
- âœ… Monitors cluster metrics in real-time using Prometheus
- âœ… Makes smart scaling decisions via AWS Lambda
- âœ… Automatically provisions/deprovisions EC2 worker nodes
- âœ… Scales seamlessly without manual intervention
- âœ… Reduces costs by 40-50% while preventing outages
- âœ… Responds to traffic spikes within 3 minutes

---

## ğŸ—ï¸ Architecture

### High-Level System Design

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        AWS Cloud (VPC)                          â”‚
â”‚                                                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚  K3s Master Node â”‚â—„â”€â”€â”€â”€â”€â”€â”€â”€â”¤  Prometheus (Metrics)       â”‚ â”‚
â”‚  â”‚   (t3.medium)    â”‚         â”‚  - CPU, Memory, Pods        â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚           â”‚                                   â”‚                 â”‚
â”‚           â”‚ Manages                          â”‚ Scrapes         â”‚
â”‚           â–¼                                   â–¼                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚           K3s Worker Nodes (Auto-Scaled)                 â”‚ â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚ â”‚
â”‚  â”‚  â”‚ Worker1 â”‚  â”‚ Worker2 â”‚  â”‚ Worker3 â”‚  â”‚ Worker4 â”‚ ...â”‚ â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚ â”‚
â”‚  â”‚         Min: 2 nodes  â”‚  Max: 10 nodes                   â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                                                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚  AWS Lambda Autoscaler (Python 3.11)                     â”‚ â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚ â”‚
â”‚  â”‚  â”‚ Every 2 min:                                       â”‚  â”‚ â”‚
â”‚  â”‚  â”‚ 1. Query Prometheus metrics                        â”‚  â”‚ â”‚
â”‚  â”‚  â”‚ 2. Check DynamoDB for locks                        â”‚  â”‚ â”‚
â”‚  â”‚  â”‚ 3. Evaluate scaling conditions                     â”‚  â”‚ â”‚
â”‚  â”‚  â”‚ 4. Launch/Terminate EC2 instances                  â”‚  â”‚ â”‚
â”‚  â”‚  â”‚ 5. Update state & send Slack alerts                â”‚  â”‚ â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚           â”‚                                                     â”‚
â”‚           â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º DynamoDB (State Management)             â”‚
â”‚           â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º Secrets Manager (K3s Token)             â”‚
â”‚           â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º ECR (Container Registry)                â”‚
â”‚           â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º SNS â†’ Slack (Notifications)             â”‚
â”‚           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º CloudWatch (Logs & Alarms)              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

          â–²
          â”‚ Triggers every 2 minutes
          â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  EventBridge     â”‚
â”‚  Scheduler       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ¯ Scaling Logic

### Scale UP Conditions (ANY must be true)
- âš ï¸ Average CPU > **70%** for 3 consecutive minutes
- âš ï¸ Pending pods exist for > **3 minutes**
- âš ï¸ Memory utilization > **75%** cluster-wide

### Scale DOWN Conditions (ALL must be true)
- âœ… Average CPU < **30%** for 10+ minutes
- âœ… No pending pods exist
- âœ… Memory utilization < **50%**

### Constraints
- **Minimum Nodes**: 2 (high availability)
- **Maximum Nodes**: 10 (cost/quota limit)
- **Scale-up**: Add 1-2 nodes at a time
- **Scale-down**: Remove 1 node at a time

### Cooldown Periods
- **After Scale-Up**: 5 minutes (prevent thrashing)
- **After Scale-Down**: 10 minutes (stability)

---

## ğŸ› ï¸ Technology Stack

| Category | Technology | Purpose |
|----------|-----------|---------|
| **IaC** | Pulumi (Python) | Infrastructure provisioning |
| **Orchestration** | K3s | Lightweight Kubernetes |
| **Cloud Provider** | AWS (EC2, Lambda, DynamoDB) | Compute & services |
| **Monitoring** | Prometheus + Grafana | Metrics collection & visualization |
| **Autoscaler** | AWS Lambda (Python 3.11) | Scaling decision engine |
| **State Management** | DynamoDB | Distributed locks & state |
| **Secret Management** | AWS Secrets Manager | Secure K3s token storage |
| **Container Registry** | Amazon ECR | Docker image storage |
| **Load Testing** | k6 | Traffic simulation |
| **Alerting** | CloudWatch + SNS + Slack | Real-time notifications |
| **CI/CD** | GitHub Actions | Automated deployment |

---

## ğŸ“ Project Structure

```
SmartScale-K3s-Autoscaler/
â”œâ”€â”€ pulumi/                      # Infrastructure as Code
â”‚   â”œâ”€â”€ __main__.py             # Main Pulumi program
â”‚   â”œâ”€â”€ Pulumi.yaml             # Project configuration
â”‚   â”œâ”€â”€ requirements.txt        # Python dependencies
â”‚   â”œâ”€â”€ vpc.py                  # VPC & networking
â”‚   â”œâ”€â”€ ec2.py                  # EC2 instances & launch templates
â”‚   â”œâ”€â”€ lambda_function.py      # Lambda resource definitions
â”‚   â”œâ”€â”€ dynamodb.py             # State management table
â”‚   â”œâ”€â”€ iam.py                  # IAM roles & policies
â”‚   â”œâ”€â”€ secrets.py              # Secrets Manager configuration
â”‚   â””â”€â”€ monitoring.py           # CloudWatch alarms & dashboards
â”‚
â”œâ”€â”€ lambda/                      # Lambda autoscaler code
â”‚   â”œâ”€â”€ autoscaler.py           # Main scaling logic
â”‚   â”œâ”€â”€ metrics_collector.py    # Prometheus query functions
â”‚   â”œâ”€â”€ ec2_manager.py          # EC2 launch/terminate operations
â”‚   â”œâ”€â”€ state_manager.py        # DynamoDB operations
â”‚   â”œâ”€â”€ k3s_helper.py           # K3s node management
â”‚   â”œâ”€â”€ slack_notifier.py       # Slack webhook integration
â”‚   â”œâ”€â”€ requirements.txt        # Lambda dependencies
â”‚   â””â”€â”€ utils.py                # Shared utilities
â”‚
â”œâ”€â”€ k3s/                         # K3s cluster configuration
â”‚   â”œâ”€â”€ master-setup.sh         # Master node initialization
â”‚   â”œâ”€â”€ worker-userdata.sh      # Worker node auto-join script
â”‚   â”œâ”€â”€ prometheus.yml          # Prometheus scrape configs
â”‚   â””â”€â”€ prometheus-deployment.yaml  # Prometheus K8s manifest
â”‚
â”œâ”€â”€ monitoring/                  # Observability configs
â”‚   â”œâ”€â”€ grafana-dashboards/
â”‚   â”‚   â”œâ”€â”€ cluster-overview.json
â”‚   â”‚   â”œâ”€â”€ autoscaler-metrics.json
â”‚   â”‚   â””â”€â”€ cost-tracking.json
â”‚   â”œâ”€â”€ cloudwatch-alarms.json  # Alarm definitions
â”‚   â””â”€â”€ alert-rules.yml         # Prometheus alert rules
â”‚
â”œâ”€â”€ demo-app/                    # Sample application
â”‚   â”œâ”€â”€ app.py                  # Flask API
â”‚   â”œâ”€â”€ Dockerfile              # Container definition
â”‚   â”œâ”€â”€ deployment.yaml         # K8s deployment manifest
â”‚   â””â”€â”€ requirements.txt        # App dependencies
â”‚
â”œâ”€â”€ tests/                       # Testing suite
â”‚   â”œâ”€â”€ load-test.js            # k6 load testing script
â”‚   â”œâ”€â”€ test-scale-up.sh        # Scale-up scenario test
â”‚   â”œâ”€â”€ test-scale-down.sh      # Scale-down scenario test
â”‚   â””â”€â”€ test-plan.md            # Testing strategy
â”‚
â”œâ”€â”€ .github/                     # CI/CD workflows
â”‚   â””â”€â”€ workflows/
â”‚       â”œâ”€â”€ deploy.yml          # Deployment pipeline
â”‚       â””â”€â”€ test.yml            # Automated testing
â”‚
â”œâ”€â”€ docs/                        # Documentation
â”‚   â”œâ”€â”€ architecture.md         # Detailed architecture
â”‚   â”œâ”€â”€ scaling-algorithm.md    # Algorithm explanation
â”‚   â”œâ”€â”€ runbook.md              # Operational procedures
â”‚   â”œâ”€â”€ troubleshooting.md      # Common issues & fixes
â”‚   â””â”€â”€ diagrams/               # Architecture diagrams
â”‚
â”œâ”€â”€ .gitignore                   # Git ignore rules
â”œâ”€â”€ README.md                    # This file
â””â”€â”€ SETUP.md                     # Step-by-step setup guide
```

---

## ğŸš€ Quick Start

### Prerequisites
- AWS Account with Free Tier access
- AWS CLI configured (`aws configure`)
- Python 3.11+
- Pulumi CLI installed
- kubectl installed
- Docker installed

### Installation Steps

1. **Clone the repository**
   ```bash
   git clone <your-repo-url>
   cd k3-scale
   ```

2. **Install Pulumi dependencies**
   ```bash
   cd pulumi
   python -m venv venv
   source venv/bin/activate  # On Windows: venv\Scripts\activate
   pip install -r requirements.txt
   ```

3. **Configure Pulumi stack**
   ```bash
   pulumi login
   pulumi stack init dev
   pulumi config set aws:region us-east-1
   pulumi config set slackWebhookUrl <your-webhook-url> --secret
   ```

4. **Deploy infrastructure**
   ```bash
   pulumi up
   ```

5. **Set up K3s cluster**
   ```bash
   cd ../k3s
   ./master-setup.sh
   ```

6. **Deploy demo application**
   ```bash
   cd ../demo-app
   kubectl apply -f deployment.yaml
   ```

7. **Run load tests**
   ```bash
   cd ../tests
   k6 run load-test.js
   ```

---

## ğŸ“Š Key Prometheus Queries

### CPU Utilization
```promql
avg(rate(node_cpu_seconds_total{mode!="idle"}[5m])) * 100
```

### Memory Utilization
```promql
(1 - avg(node_memory_MemAvailable_bytes / node_memory_MemTotal_bytes)) * 100
```

### Pending Pods
```promql
sum(kube_pod_status_phase{phase="Pending"})
```

### Node Count
```promql
count(kube_node_info)
```

---

## ğŸ” Security Features

- âœ… K3s join token stored in AWS Secrets Manager (encrypted)
- âœ… IAM roles with least-privilege policies
- âœ… Prometheus endpoint secured with authentication
- âœ… EC2 volumes encrypted at rest
- âœ… Inter-node communication encrypted
- âœ… Security groups restrict access to necessary ports only
- âœ… No hardcoded credentials in code

---

## ğŸ’° Cost Optimization

### Before Autoscaler
- **Fixed 5 nodes** running 24/7
- **Monthly cost**: ~1.2 lakh BDT
- **Utilization**: 20-30% off-peak (12 hours/day)
- **Wasted cost**: ~60,000 BDT/month

### After Autoscaler
- **Dynamic scaling**: 2-10 nodes based on demand
- **Expected monthly cost**: ~60,000-70,000 BDT
- **Savings**: 40-50% reduction
- **Improved reliability**: No capacity-related outages

---

## ğŸ“ˆ Monitoring & Alerting

### CloudWatch Dashboards
- Real-time node count and scaling events
- CPU/Memory utilization trends (24-hour view)
- Pending pods and pod distribution
- Lambda execution metrics
- Cost tracking

### Slack Notifications
- ğŸŸ¢ Scale-up initiated (nodes added, reason, new total)
- ğŸ”µ Scale-down initiated (node drained, reason, new total)
- ğŸ”´ Scaling failures with error details
- âš ï¸ Cluster at max capacity warning
- âœ… Scaling operation completed

---

## ğŸ§ª Testing Strategy

### Load Testing (k6)
- Simulate gradual traffic increase (0 â†’ 10,000 RPS)
- Flash sale scenario (instant spike)
- Sustained high load (30+ minutes)

### Scale-Up Testing
- Verify nodes launch within 3 minutes
- Confirm automatic K3s cluster join
- Check pod distribution across new nodes

### Scale-Down Testing
- Verify graceful pod drainage
- Confirm no service disruption
- Test PodDisruptionBudget respect

### Failure Scenarios
- Lambda timeout during scaling
- EC2 quota exceeded
- Prometheus unavailable
- DynamoDB lock stuck
- Node join failure

---

## ğŸ‘¥ Team & Contributions

**Developer**: [Your Name]  
**Project Duration**: Dec 16, 2024 - Jan 15, 2025  
**Institution**: Poridhi.io System Design Batch 1

---

## ğŸ“š Documentation

- [Architecture Details](docs/architecture.md)
- [Scaling Algorithm](docs/scaling-algorithm.md)
- [Operational Runbook](docs/runbook.md)
- [Troubleshooting Guide](docs/troubleshooting.md)

---

## ğŸ† Bonus Features Implemented

- [ ] Multi-AZ node distribution
- [ ] Spot instance integration
- [ ] Predictive scaling (historical analysis)
- [ ] Custom application metrics (queue depth, latency)
- [ ] GitOps with FluxCD/ArgoCD
- [ ] Real-time cost tracking dashboard

---

## ğŸ“ Development Log

Regular commits tracking progress every 4-5 hour lab session:

- **Session 1** (Dec XX): Initial setup, folder structure
- **Session 2** (Dec XX): Pulumi infrastructure base
- **Session 3** (Dec XX): Lambda autoscaler core logic
- **Session 4** (Dec XX): K3s cluster setup & Prometheus
- **Session 5** (Dec XX): Integration testing
- **Session 6** (Dec XX): Monitoring & alerting
- **Session 7** (Dec XX): Load testing & optimization
- **Session 8** (Dec XX): Documentation & final demo

---

## ğŸ“„ License

This project is developed as part of the Poridhi.io System Design Final Exam.

---

## ğŸ”— References

- [K3s Documentation](https://docs.k3s.io/)
- [AWS Lambda Best Practices](https://docs.aws.amazon.com/lambda/latest/dg/best-practices.html)
- [Prometheus Query Examples](https://prometheus.io/docs/prometheus/latest/querying/examples/)
- [Pulumi AWS Provider](https://www.pulumi.com/registry/packages/aws/)
- [k6 Load Testing](https://k6.io/docs/)

---

**ğŸš€ "Scale Smart. Automate Everything. Document Clearly."**
